#!/usr/bin/env python3
import argparse
import json
import pika
import waggle.protocol.v0 as protocol


def message_handler(ch, method, properties, body):
    for message in protocol.unpack_waggle_packets(body):
        for datagram in protocol.unpack_datagrams(message['body']):
            for sensorgram in protocol.unpack_sensorgrams(datagram['body']):
                # annotate sensorgram with message and datagram info
                sensorgram['sender_id'] = message['sender_id']
                sensorgram['sender_sub_id'] = message['sender_sub_id']
                sensorgram['plugin_id'] = datagram['plugin_id']
                sensorgram['plugin_major_version'] = datagram['plugin_major_version']
                sensorgram['plugin_minor_version'] = datagram['plugin_minor_version']
                sensorgram['plugin_instance'] = datagram['plugin_instance']
                print(json.dumps(sensorgram, sort_keys=True))

    ch.basic_ack(delivery_tag=method.delivery_tag)


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--url', default='amqp://localhost')
    parser.add_argument('node_id')
    args = parser.parse_args()

    queue = 'to-node-{}'.format(args.node_id)

    connection = pika.BlockingConnection(pika.URLParameters(args.url))
    channel = connection.channel()

    channel.queue_declare(queue=queue, durable=True)
    channel.basic_consume(message_handler, queue)
    channel.start_consuming()


if __name__ == '__main__':
    main()
#
#
#
# from cassandra.cluster import Cluster
# from datetime import datetime
# import pika
# import os
# import binascii
#
#
# cluster = Cluster(contact_points=['cassandra'])
# session = cluster.connect('waggle')
# query = 'INSERT INTO sensor_data_raw (node_id, date, plugin_name, plugin_version, plugin_instance, timestamp, parameter, data) VALUES (?, ?, ?, ?, ?, ?, ?, ?)'
# prepared = session.prepare(query)
#
#
# def process_message(ch, method, properties, body):
#     versionStrings = properties.app_id.split(':')
#     sampleDatetime = datetime.utcfromtimestamp(float(properties.timestamp) / 1000.0)
#     sampleDate = sampleDatetime.strftime('%Y-%m-%d')
#     # TODO Validate / santize node_id here.
#     node_id = properties.reply_to[-12:].lower()
#     plugin_name = versionStrings[0]
#     plugin_version = versionStrings[1]
#     plugin_instance = '0' if (len(versionStrings) < 3) else versionStrings[2]
#     timestamp = int(properties.timestamp)
#     parameter = properties.type
#     data = binascii.hexlify(body).decode()
#
#     session.execute(prepared, (node_id, sampleDate, plugin_name, plugin_version, plugin_instance, timestamp, parameter, data))
#
#     ch.basic_ack(delivery_tag=method.delivery_tag)
#     print(node_id, timestamp, plugin_name, plugin_version, parameter, flush=True)
#
#
# connection = pika.BlockingConnection(pika.ConnectionParameters(
#     host=RABBITMQ_HOST,
#     port=RABBITMQ_PORT,
#     virtual_host=BEEHIVE_DEPLOYMENT,
#     credentials=pika.PlainCredentials(
#         username=RABBITMQ_USERNAME,
#         password=RABBITMQ_PASSWORD,
#     ),
#     connection_attempts=10,
#     retry_delay=3.0))
#
# channel = connection.channel()
# # channel.basic_qos(prefetch_count=1)
#
# channel.basic_consume(process_message, queue='db-raw')
# channel.start_consuming()
